{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import (StandardScaler,\n",
    "    LabelEncoder, OneHotEncoder)\n",
    "\n",
    "data_dir = '../data'\n",
    "filename = 'AB_NYC_2019.csv'\n",
    "data_path = os.path.join(data_dir, filename)\n",
    "\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['last_review'] = pd.to_datetime(df['last_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "no_hostname = df[df['host_name'].isnull()]\n",
    "df.drop(index=no_hostname.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "no_name = df[df['name'].isnull()]\n",
    "df.drop(index=no_name.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "no_info_cond = df['number_of_reviews'] == 0 & \\\n",
    "               df['last_review'].isnull() & \\\n",
    "               df['reviews_per_month'].isnull()\n",
    "no_info_sample = df[no_info_cond]\n",
    "\n",
    "df.drop(index=no_info_sample.index, inplace=True)\n",
    "\n",
    "df.index = range(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract_date(data):\n",
    "    return data.dt.year, data.dt.month, data.dt.day\n",
    "\n",
    "year, month, day = extract_date(df['last_review'])\n",
    "\n",
    "df[['review_year', 'review_month',\n",
    "    'review_day']] = pd.DataFrame({'year': year,\n",
    "                        'month': month,\n",
    "                        'day': day})\n",
    "\n",
    "df.drop(columns='last_review', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# FE\n",
    "df['year_available'] = df['availability_365'] == 365\n",
    "df['review_period'] = df['number_of_reviews'] / df['reviews_per_month']\n",
    "df['is_rare_type'] = df['room_type'] == 'Shared room'\n",
    "df['reviews_per_host'] = df['number_of_reviews'] / df['calculated_host_listings_count']\n",
    "df['min_available'] = df['minimum_nights'] * df['availability_365']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target = df['price']\n",
    "df.drop(columns=['price'], inplace=True)\n",
    "\n",
    "target.replace(to_replace=0, value=target.mean(), inplace=True)\n",
    "target = np.log(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_df = df.select_dtypes(include=np.number)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(df[num_df.columns])\n",
    "df[num_df.columns] = pd.DataFrame(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "to_encode = ['room_type', 'neighbourhood_group']\n",
    "\n",
    "for col in to_encode:\n",
    "    le = LabelEncoder()\n",
    "    ohe = OneHotEncoder(categories='auto')\n",
    "    labeled = le.fit_transform(df[col])\n",
    "    labeled = labeled.reshape(len(labeled), 1)\n",
    "    encoded = ohe.fit_transform(labeled).toarray()\n",
    "    encoded_df = pd.DataFrame(\n",
    "        encoded, columns=['is_'+cat for cat in le.classes_]\n",
    "    )\n",
    "    df = df.join(encoded_df)\n",
    "\n",
    "df.drop(columns=to_encode, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Useless unique identifier\n",
    "df.drop(columns='id', inplace=True)\n",
    "# Have to make specific FE for sentences\n",
    "df.drop(columns='name', inplace=True)\n",
    "\n",
    "to_label = ['host_name', 'neighbourhood']\n",
    "\n",
    "for col in to_label:\n",
    "    le = LabelEncoder()\n",
    "    labeled = le.fit_transform(df[col])\n",
    "    df[col+'_label'] = labeled\n",
    "\n",
    "df.drop(columns=to_label, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Neural network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Activations and their derivatives\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid_backward(da, x):\n",
    "    sig = sigmoid(x)\n",
    "    return da * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(da, x):\n",
    "    dx = np.array(da, copy = True)\n",
    "    dx[x <= 0] = 0\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "architecture_list = (\n",
    "    {'input': df.shape[1], 'output': 40, 'activation': relu},\n",
    "    {'input': 40, 'output': 50, 'activation': relu},\n",
    "    {'input': 50, 'output': 50, 'activation': relu},\n",
    "    {'input': 50, 'output': 20, 'activation': relu},\n",
    "    {'input': 20, 'output': 1, 'activation': sigmoid}\n",
    ")\n",
    "\n",
    "backwards = {\n",
    "    'relu': relu_backward,\n",
    "    'sigmoid': sigmoid_backward\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def init_layers(architecture, seed=1):\n",
    "    np.random.seed(seed)\n",
    "    params_values = {}\n",
    "    \n",
    "    for idx, layer in enumerate(architecture):\n",
    "        layer_idx = idx + 1\n",
    "        layer_input = layer['input']\n",
    "        layer_output = layer['output']\n",
    "        # Weight matrix W and bias vector b initialization\n",
    "        params_values['W_' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output, layer_input) * 0.1\n",
    "        params_values['b_' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output, 1) * 0.1\n",
    "        \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation):\n",
    "    # calculation of the input value for the activation function\n",
    "    # W * A + b\n",
    "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "    # return of calculated activation A and the intermediate Z matrix\n",
    "    return activation(Z_curr), Z_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_forward_propagation(X, params_values, architecture):\n",
    "    # memory for information needed for a backward step\n",
    "    memory = {}\n",
    "    # X vector is the activation for layer 0â€Š\n",
    "    A_curr = X\n",
    "    \n",
    "    # iteration over network layers\n",
    "    for idx, layer in enumerate(architecture):\n",
    "        # we number network layers from 1\n",
    "        layer_idx = idx + 1\n",
    "        # transfer the activation from the previous iteration\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        # extraction of the matrix W, vector b and\n",
    "        # activation function for the current layer\n",
    "        W_curr = params_values['W_' + str(layer_idx)]\n",
    "        b_curr = params_values['b_' + str(layer_idx)]\n",
    "        activation = layer['activation']\n",
    "                \n",
    "        # calculation of activation for the current layer\n",
    "        A_curr, Z_curr = single_layer_forward_propagation(\n",
    "            A_prev, W_curr, b_curr, activation\n",
    "        )\n",
    "        \n",
    "        # saving calculated values in the memory\n",
    "        memory['A_' + str(idx)] = A_prev\n",
    "        memory['Z_' + str(layer_idx)] = Z_curr\n",
    "    \n",
    "    # return of prediction vector and a dictionary for backward\n",
    "    return A_curr, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean squared error\n",
    "def mse(y_hat, y):\n",
    "    m = y_hat.shape[1]\n",
    "    result = 1 / m * np.sum(y_hat**2 - y**2)\n",
    "    return np.squeeze(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_backward_propagation(dA_curr, W_curr,\n",
    "                                      b_curr, Z_curr, A_prev, activation):\n",
    "    # number of examples\n",
    "    m = A_prev.shape[1]\n",
    "    # selection of activation function\n",
    "    backward_activation = backwards[activation]\n",
    "    \n",
    "    # calculation of the activation function derivative\n",
    "    dZ_curr = backward_activation(dA_curr, Z_curr)\n",
    "    \n",
    "    # derivative of the matrix W, vector b\n",
    "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "    \n",
    "    # derivative of the matrix A_prev\n",
    "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_backward_propagation(y_hat, y, memory, params_values, architecture):\n",
    "    grads_values = {}\n",
    "    # number of examples\n",
    "    m = y.shape[1]\n",
    "    # a hack ensuring the same shape of the prediction vector and labels vector\n",
    "    y = y.reshape(y_hat.shape)\n",
    "    \n",
    "    # initiation of gradient descent algorithm\n",
    "    dA_prev = -(np.divide(y, y_hat) - np.divide(1 - y, 1 - y_hat));\n",
    "    \n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(architecture))):\n",
    "        # we number network layers from 1\n",
    "        layer_idx_curr = layer_idx_prev + 1\n",
    "        # extraction of the activation function for the current layer\n",
    "        activation = layer['activation']\n",
    "        \n",
    "        dA_curr = dA_prev\n",
    "        A_prev = memory['A_' + str(layer_idx_prev)]\n",
    "        Z_curr = memory['Z_' + str(layer_idx_curr)]\n",
    "        \n",
    "        W_curr = params_values['W_' + str(layer_idx_curr)]\n",
    "        b_curr = params_values['b_' + str(layer_idx_curr)]\n",
    "        \n",
    "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
    "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activation\n",
    "        )\n",
    "        \n",
    "        grads_values['dW_' + str(layer_idx_curr)] = dW_curr\n",
    "        grads_values['db_' + str(layer_idx_curr)] = db_curr\n",
    "    \n",
    "    return grads_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(params_values, grads_values, architecture, learning_rate):\n",
    "    for layer_idx, layer in enumerate(architecture, 1):\n",
    "        w_updating = learning_rate * grads_values['dW_' + str(layer_idx)]\n",
    "        params_values['W_' + str(layer_idx)] -= w_updating\n",
    "        b_updating = learning_rate * grads_values['db_' + str(layer_idx)]\n",
    "        params_values['b_' + str(layer_idx)] -= b_updating\n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y, architecture, epochs, learning_rate,\n",
    "          verbose=False, callback=None):\n",
    "    # initialization of neural net parameters\n",
    "    params_values = init_layers(architecture, 2)\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # step forward\n",
    "        y_hat, memory = full_forward_propagation(x, params_values, architecture)\n",
    "        \n",
    "        cost = mse(y_hat, y)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        # step backward - calculating gradient\n",
    "        grads_values = full_backward_propagation(\n",
    "            y_hat, y, memory, params_values, architecture\n",
    "        )\n",
    "\n",
    "        # updating model state\n",
    "        params_values = update_params(\n",
    "            params_values, grads_values, architecture, learning_rate\n",
    "        )\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            if verbose:\n",
    "                print('Iteration: {:05} - cost: {:.5f}'.format(i, cost))\n",
    "            if callback is not None :\n",
    "                callback(i, params_values)\n",
    "            \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom dense network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "x, x_test, y, y_test = train_test_split(\n",
    "    df, target, test_size=0.2, random_state=0, shuffle=True\n",
    ")\n",
    "# Train-val split\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x, y, train_size=0.8, random_state=0, shuffle=True\n",
    ")\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "epochs = 1000\n",
    "lr = 0.02\n",
    "params_values = train(np.transpose(x_train),\n",
    "    np.transpose(y_train.to_numpy().reshape((y_train.shape[0], 1))),\n",
    "    architecture_list, epochs, lr)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "y_test_hat, _ = full_forward_propagation(np.transpose(x_test),\n",
    "                                         params_values, architecture_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mse_test = mse(y_test_hat,\n",
    "    np.transpose(y_test.to_numpy().reshape((y_test.shape[0], 1))))\n",
    "print('Test set MSE: {:.3f}'.format(mse_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}